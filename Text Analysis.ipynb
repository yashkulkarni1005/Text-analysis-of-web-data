{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0tNZZVHkFN8",
        "outputId": "a6a49a88-b51a-4c33-fd36-a4a3ea8f42c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import os\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeR6SgNxk4th",
        "outputId": "298f7aea-4a51-4c89-c4ef-7343e3d35283"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##StopWords Preprocessing\n",
        "inputs = []\n",
        "for file in os.listdir(\"/content/drive/MyDrive/sentiment Analysis/StopWords\"):\n",
        "    if file.endswith(\".txt\"):\n",
        "        inputs.append(os.path.join(\"/content/drive/MyDrive/sentiment Analysis/StopWords\", file))\n",
        "\n",
        "\n",
        "# concatanate all txt files in a file called merged_file.txt\n",
        "with open('/content/drive/MyDrive/sentiment Analysis/StopWords/StopWordMerged_file.txt', 'w') as outfile:\n",
        "    for fname in inputs:\n",
        "        with open(fname, encoding=\"utf-8\", errors='ignore') as infile:\n",
        "            outfile.write(infile.read())\n",
        "\n",
        "\n",
        "\n",
        "sw = open('/content/drive/MyDrive/sentiment Analysis/StopWords/StopWordMerged_file.txt', 'r')\n",
        "stopword_data=sw.read()\n",
        "stopword_text=stopword_data.split('\\n')\n",
        "stopword_text\n",
        "\n",
        "clean_text_stopwords=[]\n",
        "def to_lower_case_stopwords(data):\n",
        "  for words in stopword_text:\n",
        "    clean_text_stopwords.append(str.lower(words))\n",
        "to_lower_case_stopwords(stopword_text)\n"
      ],
      "metadata": {
        "id": "o3LLqBIWlT5k"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Positive words and Negative words Preprocessing\n",
        "pw = open('/content/drive/MyDrive/sentiment Analysis/MasterDictionary/positive-words.txt', 'r')\n",
        "positive_word_data=pw.read()\n",
        "positive_words=positive_word_data.split('\\n')\n",
        "\n",
        "nw = open('/content/drive/MyDrive/sentiment Analysis/MasterDictionary/negative-words.txt', 'r',encoding=\"utf-8\")\n",
        "negative_word_data=nw.read()\n",
        "negative_words=negative_word_data.split('\\n')"
      ],
      "metadata": {
        "id": "Q5EOonKKmIqo"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Text Analysis function\n",
        "def text_analysis(file):\n",
        "  f = open(file, 'r')\n",
        "  data=f.read()\n",
        "  raw_text=data.split('\\n')\n",
        "\n",
        "  ##lower case text conversion\n",
        "  clean_text_1=[]\n",
        "  def to_lower_case(data):\n",
        "    for words in raw_text:\n",
        "      clean_text_1.append(str.lower(words))\n",
        "  to_lower_case(raw_text)\n",
        "\n",
        "  #Tokenization of words in text\n",
        "  clean_text_2=[word_tokenize(i) for i in clean_text_1]\n",
        "\n",
        "  #Cleaning of punctuations in text\n",
        "  clean_text_3=[]\n",
        "  for words in clean_text_2:\n",
        "    clean=[]\n",
        "    for w in words:\n",
        "      res=re.sub(r'[^\\w\\s]',\"\",w)\n",
        "      if res != \"\":\n",
        "        clean.append(res)\n",
        "    clean_text_3.append(clean)\n",
        "\n",
        "  #Cleaning of stopwords in text\n",
        "  clean_text_4=[]\n",
        "  for  words in clean_text_3:\n",
        "    w=[]\n",
        "    for word in words:\n",
        "      if not word in clean_text_stopwords:\n",
        "\n",
        "        w.append(word)\n",
        "    clean_text_4.append(w)\n",
        "\n",
        "  preprocessed_data=clean_text_4\n",
        "\n",
        "  #Calculation of positive_score,negative_score,word_count,polarity_score&subjectivity_score\n",
        "  positive_score=0\n",
        "  negative_score=0\n",
        "  neutral_score=0\n",
        "\n",
        "  for words in preprocessed_data:\n",
        "    for word in words:\n",
        "      if word in positive_words:\n",
        "        positive_score+=1\n",
        "      elif word in negative_words:\n",
        "        negative_score+=1\n",
        "      else:\n",
        "        neutral_score+=1\n",
        "\n",
        "  total_word_count_cleaned=positive_score+negative_score+neutral_score\n",
        "\n",
        "  polarity_score=(positive_score-negative_score)/((positive_score+negative_score)+0.000001)\n",
        "\n",
        "  subjectivity_score=(positive_score+negative_score)/((total_word_count_cleaned)+0.000001)\n",
        "\n",
        "\n",
        "\n",
        "  num_of_sentences=len(clean_text_3)\n",
        "\n",
        "  word_count=0\n",
        "  for words in clean_text_3:\n",
        "    for word in words:\n",
        "      word_count+=1\n",
        "\n",
        "  #Calculation of Average Sentence Length\n",
        "  avg_sen_len=word_count/num_of_sentences\n",
        "\n",
        "  syllable_count=[]\n",
        "  for words in clean_text_3:\n",
        "    for word in words:\n",
        "      count=0\n",
        "      for char in word:\n",
        "        if char in 'aeiou':\n",
        "          count+=1\n",
        "      if word[-2:]=='ed' or word[-2:]=='es':\n",
        "        count-=1\n",
        "      syllable_count.append(count)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # Calculation of syllable_per_word, complex_word_count, percentage_of_complex_words & fog index\n",
        "  syllable_per_word=sum(syllable_count)/len(syllable_count)\n",
        "\n",
        "\n",
        "  complex_words_syllable=[i for i in syllable_count if i>2]\n",
        "  complex_word_count=len(complex_words_syllable)\n",
        "  percentage_of_complex_words=complex_word_count/word_count\n",
        "\n",
        "  fog_index=0.4*(avg_sen_len+percentage_of_complex_words)\n",
        "\n",
        "  ##Calculation of Average Word length\n",
        "  char_count=0\n",
        "  for words in clean_text_3:\n",
        "    for word in words:\n",
        "      char_count+=len(word)\n",
        "\n",
        "  avg_word_len=char_count/word_count\n",
        "\n",
        "\n",
        "  #Calculation of personel pronoun count\n",
        "  personel_pronoun_text=[word_tokenize(i) for i in raw_text]\n",
        "  # import re\n",
        "  personel_pronoun_text_3=[]\n",
        "\n",
        "  for words in personel_pronoun_text:\n",
        "    clean=[]\n",
        "    for w in words:\n",
        "      res=re.sub(r'[^\\w\\s]',\"\",w)\n",
        "      if res != \"\":\n",
        "        clean.append(res)\n",
        "    personel_pronoun_text_3.append(clean)\n",
        "\n",
        "\n",
        "  personel_pronoun_text_3_updated=[]\n",
        "  for words in personel_pronoun_text_3:\n",
        "    for word in words:\n",
        "      personel_pronoun_text_3_updated.append(word)\n",
        "\n",
        "  personel_pronouns=[\"I\",\"we\",\"my\",\"ours\",\"us\"]\n",
        "  personel_pronouns_count=0\n",
        "  for i in personel_pronouns:\n",
        "    personel_pronouns_count+=personel_pronoun_text_3_updated.count(i)\n",
        "\n",
        "\n",
        "  # print(\"1.Positive Score:\",positive_score)\n",
        "  # print(\"2.Negative_Score:\",negative_score)\n",
        "  # print(\"3.Polarity_Score:\",polarity_score)\n",
        "  # print(\"4.Subjectivity_Score:\",subjectivity_score)\n",
        "  # print(\"5.Average Sentence Length:\",avg_sen_len)\n",
        "  # print(\"6.Percentage of complex words:\",percentage_of_complex_words*100)\n",
        "  # print(\"7.Fog Index:\",fog_index)\n",
        "  # print(\"8.Average number of words per sentence:\",avg_sen_len)\n",
        "  # print(\"9.Complex Word Count\",complex_word_count)\n",
        "  # print(\"10.Word Count:\",total_word_count_cleaned)\n",
        "  # print(\"11.Syllable per word:\",syllable_per_word)\n",
        "  # print(\"12.Personel Pronouns:\",personel_pronouns_count)\n",
        "  # print(\"13.Average Word Length:\",avg_word_len)\n",
        "\n",
        "  return positive_score,negative_score,polarity_score,subjectivity_score,avg_sen_len,percentage_of_complex_words*100,fog_index,avg_sen_len,complex_word_count,total_word_count_cleaned,syllable_per_word,personel_pronouns_count,avg_word_len"
      ],
      "metadata": {
        "id": "BJL5SDd9kdMK"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# def text_analysis(file):\n",
        "#   f = open(file, 'r')\n",
        "#   data=f.read()\n",
        "#   raw_text=data.split('\\n')\n",
        "\n",
        "\n",
        "#   clean_text_1=[]\n",
        "#   def to_lower_case(data):\n",
        "#     for words in raw_text:\n",
        "#       clean_text_1.append(str.lower(words))\n",
        "#   to_lower_case(raw_text)\n",
        "\n",
        "#   clean_text_2=[word_tokenize(i) for i in clean_text_1]\n",
        "\n",
        "#   clean_text_3=[]\n",
        "\n",
        "#   for words in clean_text_2:\n",
        "#     clean=[]\n",
        "#     for w in words:\n",
        "#       res=re.sub(r'[^\\w\\s]',\"\",w)\n",
        "#       if res != \"\":\n",
        "#         clean.append(res)\n",
        "#     clean_text_3.append(clean)\n",
        "\n",
        "\n",
        "#   clean_text_4=[]\n",
        "#   for  words in clean_text_3:\n",
        "#     w=[]\n",
        "#     for word in words:\n",
        "#       if not word in clean_text_stopwords:\n",
        "\n",
        "#         w.append(word)\n",
        "#     clean_text_4.append(w)\n",
        "\n",
        "#   preprocessed_data=clean_text_4\n",
        "\n",
        "#   positive_score=0\n",
        "#   negative_score=0\n",
        "#   neutral_score=0\n",
        "\n",
        "#   for words in preprocessed_data:\n",
        "#     for word in words:\n",
        "#       if word in positive_words:\n",
        "#         positive_score+=1\n",
        "#       elif word in negative_words:\n",
        "#         negative_score+=1\n",
        "#       else:\n",
        "#         neutral_score+=1\n",
        "\n",
        "#   total_word_count_cleaned=positive_score+negative_score+neutral_score\n",
        "\n",
        "#   polarity_score=(positive_score-negative_score)/((positive_score+negative_score)+0.000001)\n",
        "\n",
        "#   subjectivity_score=(positive_score+negative_score)/((total_word_count_cleaned)+0.000001)\n",
        "\n",
        "\n",
        "\n",
        "#   num_of_sentences=len(clean_text_3)\n",
        "\n",
        "#   word_count=0\n",
        "#   for words in clean_text_3:\n",
        "#     for word in words:\n",
        "#       word_count+=1\n",
        "\n",
        "\n",
        "#   avg_sen_len=word_count/num_of_sentences\n",
        "\n",
        "#   syllable_count=[]\n",
        "#   for words in clean_text_3:\n",
        "#     for word in words:\n",
        "#       count=0\n",
        "#       for char in word:\n",
        "#         if char in 'aeiou':\n",
        "#           count+=1\n",
        "#       if word[-2:]=='ed' or word[-2:]=='es':\n",
        "#         count-=1\n",
        "#       syllable_count.append(count)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#   # print(\"sum\",sum(syllable_count))\n",
        "#   syllable_per_word=sum(syllable_count)/len(syllable_count)\n",
        "\n",
        "\n",
        "#   complex_words_syllable=[i for i in syllable_count if i>2]\n",
        "#   complex_word_count=len(complex_words_syllable)\n",
        "#   percentage_of_complex_words=complex_word_count/word_count\n",
        "\n",
        "#   fog_index=0.4*(avg_sen_len+percentage_of_complex_words)\n",
        "\n",
        "#   ##Average Word length\n",
        "#   char_count=0\n",
        "#   for words in clean_text_3:\n",
        "#     for word in words:\n",
        "#       char_count+=len(word)\n",
        "\n",
        "#   avg_word_len=char_count/word_count\n",
        "\n",
        "\n",
        "\n",
        "#   personel_pronoun_text=[word_tokenize(i) for i in raw_text]\n",
        "#   # import re\n",
        "#   personel_pronoun_text_3=[]\n",
        "\n",
        "#   for words in personel_pronoun_text:\n",
        "#     clean=[]\n",
        "#     for w in words:\n",
        "#       res=re.sub(r'[^\\w\\s]',\"\",w)\n",
        "#       if res != \"\":\n",
        "#         clean.append(res)\n",
        "#     personel_pronoun_text_3.append(clean)\n",
        "\n",
        "\n",
        "#   personel_pronoun_text_3_updated=[]\n",
        "#   for words in personel_pronoun_text_3:\n",
        "#     for word in words:\n",
        "#       personel_pronoun_text_3_updated.append(word)\n",
        "\n",
        "#   personel_pronouns=[\"I\",\"we\",\"my\",\"ours\",\"us\"]\n",
        "#   personel_pronouns_count=0\n",
        "#   for i in personel_pronouns:\n",
        "#     personel_pronouns_count+=personel_pronoun_text_3_updated.count(i)\n",
        "\n",
        "\n",
        "#   # print(\"1.Positive Score:\",positive_score)\n",
        "#   # print(\"2.Negative_Score:\",negative_score)\n",
        "#   # print(\"3.Polarity_Score:\",polarity_score)\n",
        "#   # print(\"4.Subjectivity_Score:\",subjectivity_score)\n",
        "#   # print(\"5.Average Sentence Length:\",avg_sen_len)\n",
        "#   # print(\"6.Percentage of complex words:\",percentage_of_complex_words*100)\n",
        "#   # print(\"7.Fog Index:\",fog_index)\n",
        "#   # print(\"8.Average number of words per sentence:\",avg_sen_len)\n",
        "#   # print(\"9.Complex Word Count\",complex_word_count)\n",
        "#   # print(\"10.Word Count:\",total_word_count_cleaned)\n",
        "#   # print(\"11.Syllable per word:\",syllable_per_word)\n",
        "#   # print(\"12.Personel Pronouns:\",personel_pronouns_count)\n",
        "#   # print(\"13.Average Word Length:\",avg_word_len)\n",
        "\n",
        "\n",
        "#   return positive_score,negative_score,polarity_score,subjectivity_score,avg_sen_len,percentage_of_complex_words*100,fog_index,avg_sen_len,complex_word_count,total_word_count_cleaned,syllable_per_word,personel_pronouns_count,avg_word_len"
      ],
      "metadata": {
        "id": "kdIwJbIRn_Lo"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Text files extraction and final output updata in Output data file\n",
        "output_file=pd.read_excel(\"/content/drive/MyDrive/sentiment Analysis/Output Data Structure.xlsx\")\n",
        "for i in output_file[\"URL_ID\"]:\n",
        "  file='/content/drive/MyDrive/sentiment Analysis/input_text_data/'+str(i)+'.txt'\n",
        "  positive_score,negative_score,polarity_score,subjectivity_score,avg_sen_len,percentage_of_complex_words,fog_index,avg_sen_len,complex_word_count,total_word_count_cleaned,syllable_per_word,personel_pronouns_count,avg_word_len=text_analysis(file)\n",
        "  output_file.loc[output_file.URL_ID==i,[\"POSITIVE SCORE\",\"NEGATIVE SCORE\",\"POLARITY SCORE\",\"SUBJECTIVITY SCORE\",\"AVG SENTENCE LENGTH\",\"PERCENTAGE OF COMPLEX WORDS\",\"FOG INDEX\",\"AVG NUMBER OF WORDS PER SENTENCE\",\"COMPLEX WORD COUNT\",\"WORD COUNT\",\"SYLLABLE PER WORD\",\"PERSONAL PRONOUNS\",\"AVG WORD LENGTH\"]]=[positive_score,negative_score,polarity_score,subjectivity_score,avg_sen_len,percentage_of_complex_words,fog_index,avg_sen_len,complex_word_count,total_word_count_cleaned,syllable_per_word,personel_pronouns_count,avg_word_len]\n",
        "  print(i)\n",
        "\n",
        "output_file.to_excel(\"Output Data Structure Updated.xlsx\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSPhHNEdp0r7",
        "outputId": "ed767d4b-9d39-43b3-e02c-60ebe39cc6af"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0wguVMv6rMFW"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0FnjUmSJvFzr"
      },
      "execution_count": 55,
      "outputs": []
    }
  ]
}